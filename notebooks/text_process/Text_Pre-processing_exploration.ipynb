{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data from newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/celsadiaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/celsadiaz/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/celsadiaz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/celsadiaz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract topics, remove duplicates and get training data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_23_08 = pd.read_pickle('../../data/news_complete_23_08_17.pkl')\n",
    "news_24_08 = pd.read_pickle('../../data/news_complete_24_08_17.pkl')\n",
    "news_12_09 = pd.read_pickle('../../data/news_complete_12_09_17.pkl')\n",
    "news = pd.concat([news_23_08,news_24_08,news_12_09])\n",
    "news.drop('keywords', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Champions-League-Qualifikation: Liverpool läss...</td>\n",
       "      <td>(Sport, 22:34)</td>\n",
       "      <td>http://spiegel.de/sport/fussball/champions-lea...</td>\n",
       "      <td>Starker Auftritt des FC Liverpool: Auch im Rüc...</td>\n",
       "      <td>Der Traum von der Champions League ist für die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Niederlande: Terrorwarnung - Rockkonzert in Ro...</td>\n",
       "      <td>(Politik, 22:28)</td>\n",
       "      <td>http://spiegel.de/politik/ausland/rotterdam-ko...</td>\n",
       "      <td>In Rotterdam hat die Polizei das Konzert der U...</td>\n",
       "      <td>Die Band verließ den Saal unter Polizeischutz:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             topic  \\\n",
       "0  Champions-League-Qualifikation: Liverpool läss...    (Sport, 22:34)   \n",
       "1  Niederlande: Terrorwarnung - Rockkonzert in Ro...  (Politik, 22:28)   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://spiegel.de/sport/fussball/champions-lea...   \n",
       "1  http://spiegel.de/politik/ausland/rotterdam-ko...   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0  Starker Auftritt des FC Liverpool: Auch im Rüc...   \n",
       "1  In Rotterdam hat die Polizei das Konzert der U...   \n",
       "\n",
       "                                                text  \n",
       "0  Der Traum von der Champions League ist für die...  \n",
       "1  Die Band verließ den Saal unter Polizeischutz:...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 866 and number of unique urls: 662, dropping duplicates: 662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: {} and number of unique urls: {}, dropping duplicates: {}\".format(len(news), \n",
    "                                                    len(news['url'].unique()), len(news.drop_duplicates(subset=['url']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_clean = news.copy()\n",
    "news_clean = news_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Extract topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_de = ['Politik', 'Meinung', 'Wirtschaft', 'Panorama', 'Sport', 'Kultur', 'Netzwelt', 'Wissenschaft', 'Gesundheit']\n",
    "topics_en = ['politics', 'opinion', 'economy', 'society', 'sport', 'culture', 'technology', 'science', 'health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_clean['topic'] = news_clean['topic'].apply(lambda x: x.split(',')[0].split('(')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_clean = news_clean[news_clean['topic'].isin(topics_de)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 topics.\n"
     ]
    }
   ],
   "source": [
    "n_topics = len(news_clean['topic'].unique())\n",
    "print(\"Found {} topics.\".format(len(news_clean['topic'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politik         175\n",
       "Panorama        130\n",
       "Sport            89\n",
       "Wirtschaft       76\n",
       "Kultur           54\n",
       "Netzwelt         29\n",
       "Wissenschaft     28\n",
       "Gesundheit       20\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(news_clean['topic']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Balance train data by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_random_index_n_times(source, n_times):\n",
    "    indices = source.index.tolist()\n",
    "    indices = np.random.choice(indices, n_times)\n",
    "    return source.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_Panorama = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Panorama']\n",
    "n_Sport = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Sport']\n",
    "n_Wirtschaft = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Wirtschaft']\n",
    "n_Kultur = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Kultur']\n",
    "n_Netzwelt = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Netzwelt']\n",
    "n_Wissenschaft = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Wissenschaft']\n",
    "n_Gesundheit = pd.Series(news_clean['topic']).value_counts()['Politik']-pd.Series(news_clean['topic']).value_counts()['Gesundheit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Panorama = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Panorama'], n_Panorama)\n",
    "df_Sport = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Sport'], n_Sport)\n",
    "df_Wirtschaft = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Wirtschaft'], n_Wirtschaft)\n",
    "df_Kultur = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Kultur'], n_Kultur)\n",
    "df_Netzwelt = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Netzwelt'], n_Netzwelt)\n",
    "df_Wissenschaft = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Wissenschaft'], n_Wissenschaft)\n",
    "df_Gesundheit = pick_random_index_n_times(news_clean[news_clean['topic'] == 'Gesundheit'], n_Gesundheit)\n",
    "complete_news = pd.concat([news_clean,df_Panorama,df_Sport,df_Wirtschaft,df_Kultur,df_Netzwelt,df_Wissenschaft,df_Gesundheit],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wissenschaft    197\n",
       "Kultur          195\n",
       "Sport           193\n",
       "Wirtschaft      189\n",
       "Panorama        184\n",
       "Netzwelt        175\n",
       "Politik         175\n",
       "Gesundheit      175\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(complete_news['topic']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Text to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_texts(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.replace('\\n','').replace('(','').replace(')','').replace(':','').replace('@','').replace(';','').replace('\\'','').replace(\"\\\"\",'').replace('?','').replace('!','').replace('/','').replace('-','').replace('.','').replace(',','').translate(remove_digits)\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary_and_vectorize_texts(list_of_texts):\n",
    "    # to avoid messing things up, I'll call all objects relating to our first model _m1\n",
    "    ldainput = [text.split() for text in list_of_texts]           # convert all strings to list of words\n",
    "    id2word = corpora.Dictionary(ldainput)                       # assign a token_id to each word\n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in ldainput] \n",
    "    return id2word, ldacorpus # represent each speech by (token_id, token_count) tuples\n",
    "\n",
    "def build_dictionary_and_vectorize_texts_wo_stopwords(list_of_texts, language):\n",
    "    # get stopwords for the given language\n",
    "    mystopwords = set(stopwords.words(language))\n",
    "    list_of_texts = [\" \".join([w for w in text.split() if w not in mystopwords]) for text in list_of_texts]\n",
    "    \n",
    "    # to avoid messing things up, I'll call all objects relating to our first model _m1\n",
    "    ldainput = [text.split() for text in list_of_texts]           # convert all strings to list of words\n",
    "    id2word = corpora.Dictionary(ldainput)                       # assign a token_id to each word\n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in ldainput] \n",
    "    return id2word, ldacorpus # represent each speech by (token_id, token_count) tuples\n",
    "\n",
    "def build_dictionary_and_vectorize_texts_wo_random_wo_common_words(list_of_texts, min_count, max_freq):\n",
    "    # to avoid messing things up, I'll call all objects relating to our first model _m1\n",
    "    ldainput = [text.split() for text in list_of_texts]           # convert all strings to list of words\n",
    "    id2word = corpora.Dictionary(ldainput)\n",
    "    id2word.filter_extremes(no_below=min_count, no_above=max_freq) \n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in ldainput] \n",
    "    return id2word, ldacorpus# represent each speech by (token_id, token_count) tuples\n",
    "\n",
    "def build_dictionary_and_vectorize_texts_w_stemmer(list_of_texts, min_count, max_freq, stemmer):\n",
    "    texts_stemmed = [\" \".join([stemmer.stem(word) for word in text.split()]) for text in list_of_texts]\n",
    "    ldainput = [text.split() for text in texts_stemmed]           # convert all strings to list of words\n",
    "    id2word = corpora.Dictionary(ldainput)\n",
    "    id2word.filter_extremes(no_below=min_count, no_above=max_freq) \n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in ldainput] \n",
    "    return id2word, ldacorpus# represent each speech by (token_id, token_count) tuples\n",
    "\n",
    "def build_dictionary_and_vectorize_texts_w_ngrams_features(list_of_texts, language, n_grams,  min_count, max_freq):\n",
    "    # get stopwords for the given language\n",
    "    enstopwords = list(set(stopwords.words('english')))\n",
    "    mystopwords = list(set(stopwords.words(language)))\n",
    "    allstopwords = enstopwords + enstopwords\n",
    "    list_of_texts = [\" \".join([w for w in text.split() if w not in allstopwords]) for text in list_of_texts]\n",
    "    \n",
    "    # get text with n_gram features\n",
    "    text_n_grams = [[\"_\".join(tup) for tup in nltk.ngrams(text.split(),n_grams)] for text in list_of_texts]\n",
    "    \n",
    "    # get combined text\n",
    "    text_combined = []\n",
    "    for a,b in zip([text.split() for text in list_of_texts],text_n_grams):\n",
    "        text_combined.append(a + b)\n",
    "    \n",
    "    id2word = corpora.Dictionary(text_combined)                       \n",
    "    id2word.filter_extremes(no_below=min_count, no_above=max_freq)\n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in text_combined] \n",
    "    return id2word, ldacorpus # represent each speech by (token_id, token_count) tuples\n",
    "\n",
    "\n",
    "\n",
    "def build_dictionary_and_vectorize_texts_w_ngrams_features_only_noun_adj(list_of_texts, language, n_grams,  min_count, max_freq):\n",
    "    # get stopwords for the given language\n",
    "    enstopwords = list(set(stopwords.words('english')))\n",
    "    mystopwords = list(set(stopwords.words(language)))\n",
    "    allstopwords = enstopwords + enstopwords\n",
    "    list_of_texts = [\" \".join([w for w in text.split() if w not in allstopwords]) for text in list_of_texts]\n",
    "    \n",
    "    # get text with n_gram features\n",
    "    text_n_grams = [[\"_\".join(tup) for tup in nltk.ngrams(text.split(),n_grams)] for text in list_of_texts]\n",
    "    \n",
    "    # get combined text NOT HELPING!!!\n",
    "    text_combined = []\n",
    "    for a,b in zip([text.split() for text in list_of_texts],text_n_grams):\n",
    "        text_combined.append(a + b)\n",
    "        \n",
    "    # consider only nouns and adjectives NOT WORKING! DO IT BEFORE DOING ANYTHING ELSE!\n",
    "    text_combined = [\" \".join(text) for text in text_combined]\n",
    "    texts_nounsadj=[]\n",
    "    for text in text_combined:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        cleantext = \"\"\n",
    "        for element in tagged:\n",
    "            if element[1] in ('NN','JJ'):\n",
    "                cleantext=cleantext+element[0]+\" \"\n",
    "        texts_nounsadj.append(cleantext)\n",
    "    \n",
    "    # INCREASE MIN_COUNT MAX_FREQ BOUNDARIES\n",
    "    texts_nounsadj = [text.split() for text in texts_nounsadj]\n",
    "    id2word = corpora.Dictionary(texts_nounsadj)                       \n",
    "    id2word.filter_extremes(no_below=min_count, no_above=max_freq)\n",
    "    ldacorpus = [id2word.doc2bow(doc) for doc in texts_nounsadj] \n",
    "    return id2word, ldacorpus # represent each speech by (token_id, token_count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_clean['text'] = news_clean['text'].apply(lambda x: clean_texts(x))\n",
    "news_clean['title'] = news_clean['title'].apply(lambda x: clean_texts(x))\n",
    "news_clean['meta_description'] = news_clean['meta_description'].apply(lambda x: clean_texts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.GermanStemmer()\n",
    "\n",
    "id2word_m1, ldacorpus_m1 = build_dictionary_and_vectorize_texts(news_clean['text'].tolist())\n",
    "id2word_m2, ldacorpus_m2 = build_dictionary_and_vectorize_texts_wo_stopwords(news_clean['text'].tolist(), 'german')\n",
    "id2word_m4, ldacorpus_m4 = build_dictionary_and_vectorize_texts_wo_random_wo_common_words(news_clean['text'].tolist(), \n",
    "                                                                                          5, 0.5)\n",
    "id2word_m5, ldacorpus_m5 = build_dictionary_and_vectorize_texts_w_stemmer(news_clean['text'].tolist(), 5, 0.5, stemmer)\n",
    "id2word_m6, ldacorpus_m6 = build_dictionary_and_vectorize_texts_w_ngrams_features(news_clean['text'].tolist(), \n",
    "                                                                                  'german', 2, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word_m7, ldacorpus_m7 = build_dictionary_and_vectorize_texts_w_ngrams_features_only_noun_adj(\n",
    "                                                                                news_clean['text'].tolist(), \n",
    "                                                                                  'german', 2, 5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train LDA topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. LDA topic modeling + removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_m2 = models.LdaModel(ldacorpus_m2, id2word=id2word_m2, num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dass\" + 0.003*\"sei\" + 0.003*\"sagte\" + 0.003*\"mehr\" + 0.003*\"wurde\" + 0.002*\"immer\" + 0.002*\"prozent\" + 0.002*\"menschen\" + 0.002*\"deutschland\" + 0.002*\"schon\"'),\n",
       " (1,\n",
       "  '0.006*\"dass\" + 0.003*\"wurde\" + 0.003*\"sagte\" + 0.003*\"deutschland\" + 0.003*\"sei\" + 0.002*\"mehr\" + 0.002*\"schon\" + 0.002*\"menschen\" + 0.001*\"usa\" + 0.001*\"seit\"'),\n",
       " (2,\n",
       "  '0.007*\"dass\" + 0.003*\"mehr\" + 0.003*\"wurde\" + 0.003*\"menschen\" + 0.002*\"sei\" + 0.002*\"euro\" + 0.002*\"schon\" + 0.002*\"zwei\" + 0.002*\"jahren\" + 0.002*\"sagte\"'),\n",
       " (3,\n",
       "  '0.004*\"dass\" + 0.004*\"sagte\" + 0.003*\"sei\" + 0.003*\"mehr\" + 0.002*\"wurde\" + 0.002*\"schon\" + 0.002*\"seit\" + 0.002*\"jahren\" + 0.002*\"menschen\" + 0.002*\"zwei\"'),\n",
       " (4,\n",
       "  '0.007*\"dass\" + 0.004*\"mehr\" + 0.004*\"sagte\" + 0.003*\"wurde\" + 0.003*\"sei\" + 0.003*\"deutschland\" + 0.002*\"immer\" + 0.002*\"zwei\" + 0.002*\"sagt\" + 0.002*\"menschen\"'),\n",
       " (5,\n",
       "  '0.006*\"dass\" + 0.004*\"mehr\" + 0.003*\"wurde\" + 0.002*\"schon\" + 0.002*\"beim\" + 0.002*\"sagte\" + 0.002*\"sei\" + 0.002*\"seit\" + 0.002*\"menschen\" + 0.002*\"jahre\"'),\n",
       " (6,\n",
       "  '0.006*\"dass\" + 0.004*\"mehr\" + 0.003*\"seit\" + 0.003*\"prozent\" + 0.002*\"deutschland\" + 0.002*\"sagte\" + 0.002*\"jahr\" + 0.002*\"gibt\" + 0.002*\"sei\" + 0.002*\"menschen\"'),\n",
       " (7,\n",
       "  '0.006*\"dass\" + 0.004*\"sei\" + 0.003*\"mehr\" + 0.003*\"sagte\" + 0.002*\"menschen\" + 0.002*\"deutschland\" + 0.002*\"euro\" + 0.002*\"zwei\" + 0.002*\"seit\" + 0.002*\"etwa\"')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Train TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize scores by most common words instead of just simply removing standard stopwords\n",
    "ldacorpus_m3 = ldacorpus_m1\n",
    "id2word_m3 = id2word_m1\n",
    "tfidfcorpus_m3 = models.TfidfModel(ldacorpus_m3)\n",
    "lda_m3 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m3[ldacorpus_m3],id2word=id2word_m3,num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"trump\" + 0.000*\"er\" + 0.000*\"sie\" + 0.000*\"oder\" + 0.000*\"polizei\" + 0.000*\"sagte\" + 0.000*\"zschäpe\" + 0.000*\"euro\" + 0.000*\"afghanistan\" + 0.000*\"diesen\"'),\n",
       " (1,\n",
       "  '0.000*\"er\" + 0.000*\"prozent\" + 0.000*\"millionen\" + 0.000*\"deutschland\" + 0.000*\"fc\" + 0.000*\"air\" + 0.000*\"sind\" + 0.000*\"so\" + 0.000*\"sie\" + 0.000*\"gegen\"'),\n",
       " (2,\n",
       "  '0.000*\"ich\" + 0.000*\"millionen\" + 0.000*\"iphone\" + 0.000*\"er\" + 0.000*\"euro\" + 0.000*\"menschen\" + 0.000*\"sie\" + 0.000*\"dollar\" + 0.000*\"milliarden\" + 0.000*\"lewis\"'),\n",
       " (3,\n",
       "  '0.000*\"türkei\" + 0.000*\"ich\" + 0.000*\"sonnenfinsternis\" + 0.000*\"er\" + 0.000*\"rooney\" + 0.000*\"iphone\" + 0.000*\"euro\" + 0.000*\"sie\" + 0.000*\"habe\" + 0.000*\"usa\"'),\n",
       " (4,\n",
       "  '0.000*\"prozent\" + 0.000*\"trump\" + 0.000*\"sie\" + 0.000*\"er\" + 0.000*\"afd\" + 0.000*\"serebrennikov\" + 0.000*\"spd\" + 0.000*\"wie\" + 0.000*\"online\" + 0.000*\"sei\"'),\n",
       " (5,\n",
       "  '0.000*\"schulz\" + 0.000*\"polizei\" + 0.000*\"er\" + 0.000*\"merkel\" + 0.000*\"euro\" + 0.000*\"sagte\" + 0.000*\"wir\" + 0.000*\"dass\" + 0.000*\"deutschland\" + 0.000*\"nordkorea\"'),\n",
       " (6,\n",
       "  '0.000*\"türkei\" + 0.000*\"er\" + 0.000*\"sie\" + 0.000*\"ich\" + 0.000*\"menschen\" + 0.000*\"deutschland\" + 0.000*\"oder\" + 0.000*\"wir\" + 0.000*\"man\" + 0.000*\"wie\"'),\n",
       " (7,\n",
       "  '0.000*\"prozent\" + 0.000*\"frauen\" + 0.000*\"wall\" + 0.000*\"ich\" + 0.000*\"er\" + 0.000*\"sie\" + 0.000*\"usa\" + 0.000*\"–\" + 0.000*\"sind\" + 0.000*\"männer\"')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. TF-IDF model + remove random & common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that are not more than 5 times or more than 50% in all documents\n",
    "tfidfcorpus_m4 = models.TfidfModel(ldacorpus_m4)\n",
    "lda_m4 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m4[ldacorpus_m4],id2word=id2word_m4,num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"frauen\" + 0.001*\"euro\" + 0.001*\"ich\" + 0.001*\"berlin\" + 0.001*\"wir\" + 0.001*\"oder\" + 0.001*\"man\" + 0.001*\"sagte\" + 0.001*\"the\" + 0.001*\"polizei\"'),\n",
       " (1,\n",
       "  '0.002*\"trump\" + 0.001*\"wir\" + 0.001*\"gegen\" + 0.001*\"fc\" + 0.001*\"bis\" + 0.001*\"sagte\" + 0.001*\"regierung\" + 0.001*\"bayern\" + 0.001*\"millionen\" + 0.001*\"deutschland\"'),\n",
       " (2,\n",
       "  '0.001*\"gegen\" + 0.001*\"deutschland\" + 0.001*\"millionen\" + 0.001*\"euro\" + 0.001*\"prozent\" + 0.001*\"zwei\" + 0.001*\"türkei\" + 0.001*\"polizei\" + 0.001*\"mehr\" + 0.001*\"barcelona\"'),\n",
       " (3,\n",
       "  '0.001*\"prozent\" + 0.001*\"film\" + 0.001*\"sei\" + 0.001*\"frauen\" + 0.001*\"ich\" + 0.001*\"frau\" + 0.001*\"menschen\" + 0.001*\"oder\" + 0.001*\"wir\" + 0.001*\"türkei\"'),\n",
       " (4,\n",
       "  '0.002*\"ich\" + 0.001*\"man\" + 0.001*\"kinder\" + 0.001*\"prozent\" + 0.001*\"oder\" + 0.001*\"menschen\" + 0.001*\"euro\" + 0.001*\"seine\" + 0.001*\"sagte\" + 0.001*\"wenn\"'),\n",
       " (5,\n",
       "  '0.001*\"prozent\" + 0.001*\"euro\" + 0.001*\"the\" + 0.001*\"spd\" + 0.001*\"wir\" + 0.001*\"trump\" + 0.001*\"habe\" + 0.001*\"polizei\" + 0.001*\"ich\" + 0.001*\"zum\"'),\n",
       " (6,\n",
       "  '0.002*\"prozent\" + 0.001*\"ich\" + 0.001*\"milliarden\" + 0.001*\"menschen\" + 0.001*\"deutschland\" + 0.001*\"sagte\" + 0.001*\"eu\" + 0.001*\"habe\" + 0.001*\"euro\" + 0.001*\"man\"'),\n",
       " (7,\n",
       "  '0.001*\"ich\" + 0.001*\"türkei\" + 0.001*\"sagt\" + 0.001*\"schulz\" + 0.001*\"seine\" + 0.001*\"millionen\" + 0.001*\"man\" + 0.001*\"habe\" + 0.001*\"zum\" + 0.001*\"trump\"')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m4.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. TF-IDF model + remove random & common words + similar lexical root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfcorpus_m5 = models.TfidfModel(ldacorpus_m5)\n",
    "lda_m5 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"prozent\" + 0.002*\"iphon\" + 0.001*\"ich\" + 0.001*\"deutsch\" + 0.001*\"air\" + 0.001*\"soll\" + 0.001*\"eu\" + 0.001*\"dann\" + 0.001*\"geg\" + 0.001*\"deutschland\"'),\n",
       " (1,\n",
       "  '0.002*\"frau\" + 0.001*\"ich\" + 0.001*\"indi\" + 0.001*\"podcast\" + 0.001*\"konn\" + 0.001*\"turkei\" + 0.001*\"oder\" + 0.001*\"spiel\" + 0.001*\"mann\" + 0.001*\"bis\"'),\n",
       " (2,\n",
       "  '0.002*\"ich\" + 0.002*\"wir\" + 0.002*\"trump\" + 0.001*\"berlin\" + 0.001*\"million\" + 0.001*\"spanisch\" + 0.001*\"barcelona\" + 0.001*\"nordkorea\" + 0.001*\"mich\" + 0.001*\"fc\"'),\n",
       " (3,\n",
       "  '0.002*\"kind\" + 0.001*\"muslim\" + 0.001*\"mensch\" + 0.001*\"prozent\" + 0.001*\"geg\" + 0.001*\"land\" + 0.001*\"afd\" + 0.001*\"man\" + 0.001*\"deutschland\" + 0.001*\"wenn\"'),\n",
       " (4,\n",
       "  '0.001*\"mensch\" + 0.001*\"euro\" + 0.001*\"prozent\" + 0.001*\"zschap\" + 0.001*\"deutsch\" + 0.001*\"googl\" + 0.001*\"schulz\" + 0.001*\"usa\" + 0.001*\"soll\" + 0.001*\"mann\"'),\n",
       " (5,\n",
       "  '0.002*\"turkei\" + 0.002*\"million\" + 0.001*\"euro\" + 0.001*\"deutsch\" + 0.001*\"fc\" + 0.001*\"deutschland\" + 0.001*\"zahl\" + 0.001*\"geg\" + 0.001*\"turkisch\" + 0.001*\"sei\"'),\n",
       " (6,\n",
       "  '0.002*\"ich\" + 0.001*\"spahn\" + 0.001*\"kauf\" + 0.001*\"trump\" + 0.001*\"kein\" + 0.001*\"konn\" + 0.001*\"milliard\" + 0.001*\"les\" + 0.001*\"usa\" + 0.001*\"trumps\"'),\n",
       " (7,\n",
       "  '0.002*\"ich\" + 0.002*\"polizei\" + 0.001*\"euro\" + 0.001*\"prozent\" + 0.001*\"polit\" + 0.001*\"jahrig\" + 0.001*\"million\" + 0.001*\"ihn\" + 0.001*\"jung\" + 0.001*\"the\"')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m5.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. TF-IDF model + remove stopwords + ignore common and random words + ngrams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfcorpus_m6 = models.TfidfModel(ldacorpus_m6)\n",
    "lda_m6 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m6[ldacorpus_m6],id2word=id2word_m6,num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"usa\" + 0.001*\"singapur\" + 0.001*\"trump\" + 0.001*\"sagte\" + 0.001*\"sei\" + 0.001*\"fc\" + 0.001*\"worden\" + 0.001*\"habe\" + 0.000*\"millionen\" + 0.000*\"selbst\"'),\n",
       " (1,\n",
       "  '0.001*\"türkei\" + 0.001*\"der_türkei\" + 0.001*\"prozent\" + 0.001*\"soll\" + 0.001*\"wurden\" + 0.001*\"wegen\" + 0.001*\"menschen\" + 0.001*\"mit_dem\" + 0.001*\"deutschland\" + 0.001*\"gruppe\"'),\n",
       " (2,\n",
       "  '0.001*\"euro\" + 0.001*\"ich\" + 0.001*\"millionen\" + 0.001*\"millionen_euro\" + 0.001*\"menschen\" + 0.001*\"wir\" + 0.001*\"prozent\" + 0.001*\"milliarden_euro\" + 0.001*\"milliarden\" + 0.001*\"gegen\"'),\n",
       " (3,\n",
       "  '0.001*\"ich\" + 0.001*\"wir\" + 0.001*\"habe\" + 0.001*\"millionen\" + 0.001*\"polizei\" + 0.001*\"k\" + 0.001*\"sagte\" + 0.001*\"uns\" + 0.001*\"fc\" + 0.001*\"laut\"'),\n",
       " (4,\n",
       "  '0.001*\"ich\" + 0.001*\"deutschland\" + 0.001*\"oder\" + 0.001*\"macron\" + 0.001*\"wir\" + 0.001*\"gegen\" + 0.001*\"nur\" + 0.001*\"zum\" + 0.001*\"euro\" + 0.000*\"männer\"'),\n",
       " (5,\n",
       "  '0.001*\"prozent\" + 0.001*\"prozent_der\" + 0.001*\"studie\" + 0.001*\"open\" + 0.001*\"oder\" + 0.001*\"frauen\" + 0.001*\"polizei\" + 0.001*\"ich\" + 0.000*\"können\" + 0.000*\"habe\"'),\n",
       " (6,\n",
       "  '0.001*\"wall\" + 0.001*\"trump\" + 0.001*\"türkei\" + 0.001*\"menschen\" + 0.001*\"ich\" + 0.001*\"sagte\" + 0.001*\"man\" + 0.001*\"usa\" + 0.001*\"land\" + 0.001*\"kinder\"'),\n",
       " (7,\n",
       "  '0.001*\"air_berlin\" + 0.001*\"air\" + 0.001*\"berlin\" + 0.001*\"lufthansa\" + 0.001*\"gegen\" + 0.001*\"frauen\" + 0.001*\"nordkorea\" + 0.001*\"deutschland\" + 0.001*\"sagt\" + 0.001*\"trump\"')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m6.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. TF-IDF model + remove stopwords + ignore common and random words + ngrams as features + only noums and adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfcorpus_m7 = models.TfidfModel(ldacorpus_m7)\n",
    "lda_m7 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m7[ldacorpus_m7],id2word=id2word_m7,num_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"polizei\" + 0.001*\"trump\" + 0.001*\"online\" + 0.001*\"man\" + 0.001*\"ich\" + 0.001*\"gegen\" + 0.001*\"b\" + 0.001*\"für_die\" + 0.001*\"sagte\" + 0.001*\"ihrer\"'),\n",
       " (1,\n",
       "  '0.001*\"polizei\" + 0.001*\"ich\" + 0.001*\"spahn\" + 0.001*\"deutschland\" + 0.001*\"man\" + 0.001*\"oder\" + 0.001*\"menschen\" + 0.001*\"ja\" + 0.001*\"sind\" + 0.001*\"zum\"'),\n",
       " (2,\n",
       "  '0.001*\"seine\" + 0.001*\"ich\" + 0.001*\"open\" + 0.001*\"habe\" + 0.001*\"sei\" + 0.001*\"trump\" + 0.001*\"sein\" + 0.001*\"nur\" + 0.001*\"sind\" + 0.001*\"dann\"'),\n",
       " (3,\n",
       "  '0.001*\"prozent\" + 0.001*\"türkei\" + 0.001*\"ich\" + 0.001*\"euro\" + 0.001*\"milliarden_euro\" + 0.001*\"peta\" + 0.001*\"soll\" + 0.001*\"milliarden\" + 0.001*\"wir\" + 0.001*\"deutschland\"'),\n",
       " (4,\n",
       "  '0.001*\"euro\" + 0.001*\"sagt\" + 0.001*\"ich\" + 0.001*\"iphone\" + 0.001*\"man\" + 0.001*\"menschen\" + 0.001*\"habe\" + 0.001*\"gegen\" + 0.001*\"oder\" + 0.001*\"dort\"'),\n",
       " (5,\n",
       "  '0.001*\"fc\" + 0.001*\"schulz\" + 0.001*\"bayern\" + 0.001*\"millionen\" + 0.001*\"ich\" + 0.001*\"wir\" + 0.001*\"dollar\" + 0.001*\"sagt\" + 0.001*\"millionen_dollar\" + 0.001*\"vielleicht\"'),\n",
       " (6,\n",
       "  '0.001*\"millionen\" + 0.001*\"sagte\" + 0.001*\"prozent\" + 0.001*\"euro\" + 0.001*\"ich\" + 0.001*\"usa\" + 0.001*\"gegen\" + 0.001*\"regierung\" + 0.001*\"trump\" + 0.001*\"soll\"'),\n",
       " (7,\n",
       "  '0.001*\"prozent\" + 0.001*\"usa\" + 0.001*\"sagte\" + 0.001*\"türkei\" + 0.001*\"nordkorea\" + 0.001*\"deutschland\" + 0.001*\"habe\" + 0.001*\"wir\" + 0.001*\"soll\" + 0.001*\"mann\"')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_m7.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
